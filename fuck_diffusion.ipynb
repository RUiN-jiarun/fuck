{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuck Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuck DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class SinosoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SinosoidalPositionalEncoding, self).__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去噪神经网络\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, device, time_dim=16):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.t_dim = time_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # 时间编码\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinosoidalPositionalEncoding(self.t_dim),\n",
    "            nn.Linear(self.t_dim, self.t_dim * 2),\n",
    "            nn.Mish(),  # 激活函数\n",
    "            nn.Linear(self.t_dim * 2, self.t_dim)\n",
    "        )\n",
    "        \n",
    "        # 中间层&输出层\n",
    "        input_dim = state_dim + self.t_dim + self.a_dim\n",
    "        self.mid_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.Mish(),  # 激活函数\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Mish(),  # 激活函数\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Mish(),  # 激活函数\n",
    "            nn.Linear(hidden_dim, action_dim)   # 输出层\n",
    "        )\n",
    "        \n",
    "        # 参数初始化\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x, time, state):\n",
    "        t_embedding = self.time_mlp(time)\n",
    "        x = torch.cat([x, state, t_embedding], dim=1)\n",
    "        x = self.mid_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuck Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, targ, weighted=1.0):\n",
    "        \"\"\"\n",
    "        pred, targ : [batch_size, action_dim]\n",
    "        \"\"\"\n",
    "        loss = self._loss(pred, targ)\n",
    "        WeightedLoss = (loss * weighted).mean()\n",
    "        return WeightedLoss\n",
    "\n",
    "\n",
    "class WeightedL1(WeightedLoss):\n",
    "    def _loss(self, pred, targ):\n",
    "        return torch.abs(pred - targ)\n",
    "\n",
    "\n",
    "class WeightedL2(WeightedLoss):\n",
    "    def _loss(self, pred, targ):\n",
    "        return F.mse_loss(pred, targ, reduction=\"none\")\n",
    "\n",
    "\n",
    "Losses = {\n",
    "    \"l1\": WeightedL1,\n",
    "    \"l2\": WeightedL2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, \n",
    "                 loss_type, \n",
    "                 beta_schedule=\"linear\",\n",
    "                 clip_denoised=True, \n",
    "                 predict_epsilon=True,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.state_dim = kwargs[\"obs_dim\"]\n",
    "        self.action_dim = kwargs[\"act_dim\"]\n",
    "        self.hidden_dim = kwargs[\"hid_dim\"]\n",
    "        self.T = kwargs[\"T\"]    # 反传多少步\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.predict_epsilon = predict_epsilon\n",
    "        self.device = torch.device(kwargs[\"device\"])\n",
    "        \n",
    "        self.model = MLP(self.state_dim, self.action_dim, self.hidden_dim, self.device).to(kwargs[\"device\"])\n",
    "        \n",
    "        # 时间离散\n",
    "        if beta_schedule == \"linear\":\n",
    "            betas = torch.linspace(0.0001, 0.02, self.T, dtype=torch.float32, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)    #\n",
    "        alphas_cumprod_prev = torch.cat([torch.tensor([1.0], dtype=torch.float32).to(self.device), alphas_cumprod[:-1]])\n",
    "        \n",
    "        # 放到buffer里方便调用参数\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        self.register_buffer(\"alphas\", alphas)\n",
    "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n",
    "        \n",
    "        # 前向过程\n",
    "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_1m_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod))\n",
    "        \n",
    "        # 反向过程\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        self.register_buffer(\"posterior_variance\", posterior_variance)\n",
    "        self.register_buffer(\"posterior_log_variance_clipped\", torch.log(torch.max(posterior_variance, torch.tensor(1e-20).to(self.device))))\n",
    "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod))\n",
    "        self.register_buffer(\"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod - 1))\n",
    "        \n",
    "        self.register_buffer(\"posterior_mean_coef1\", betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod))\n",
    "        self.register_buffer(\"posterior_mean_coef2\", (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))\n",
    "        \n",
    "        # 损失函数\n",
    "        self.loss_fn = Losses[loss_type]()\n",
    "    \n",
    "    def q_posterior(self, x_start, x, t):\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x.shape) * x_start\n",
    "            + extract(self.posterior_mean_coef2, t, x.shape) * x\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x.shape)\n",
    "        posterior_log_variance = extract(\n",
    "            self.posterior_log_variance_clipped, t, x.shape\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x, t, pred_noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x.shape) * pred_noise\n",
    "        )\n",
    "\n",
    "    def p_mean_variance(self, x, t, s):\n",
    "        pred_noise = self.model(x, t, s)\n",
    "        x_recon = self.predict_start_from_noise(x, t, pred_noise)\n",
    "        x_recon.clamp_(-1, 1)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n",
    "            x_recon, x, t\n",
    "        )\n",
    "        return model_mean, posterior_log_variance\n",
    "\n",
    "    def p_sample(self, x, t, s):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, model_log_variance = self.p_mean_variance(x, t, s)\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    def p_sample_loop(self, state, shape, *args, **kwargs):\n",
    "        device = self.device\n",
    "        batch_size = state.shape[0]\n",
    "        x = torch.randn(shape, device=device, requires_grad=False)\n",
    "\n",
    "        for i in reversed(range(0, self.T)):\n",
    "            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n",
    "            x = self.p_sample(x, t, state)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def sample(self, state, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        state : [batch_size, state_dim]\n",
    "        \"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        shape = [batch_size, self.action_dim]\n",
    "        action = self.p_sample_loop(state, shape, *args, **kwargs)\n",
    "        return action.clamp_(-1, 1)\n",
    "\n",
    "    # ------------------------------------------ training ------------------------------------------#\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "\n",
    "        sample = (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + extract(self.sqrt_1m_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def p_losses(self, x_start, state, t, weights=1.0):\n",
    "        noise = torch.randn_like(x_start)\n",
    "\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "\n",
    "        x_recon = self.model(x_noisy, t, state)\n",
    "\n",
    "        assert noise.shape == x_recon.shape\n",
    "\n",
    "        if self.predict_epsilon:\n",
    "            loss = self.loss_fn(x_recon, noise, weights)\n",
    "        else:\n",
    "            loss = self.loss_fn(x_recon, x_start, weights)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def loss(self, x, state, weights=1.0):\n",
    "        batch_size = len(x)\n",
    "        t = torch.randint(0, self.T, (batch_size,), device=self.device).long()\n",
    "        return self.p_losses(x, state, t, weights)\n",
    "    \n",
    "    def forward(self, state, *args, **kwargs):\n",
    "        return self.sample(state, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.035905361175537\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"  # cuda\n",
    "x = torch.randn(256, 2).to(device)  # Batch, action_dim\n",
    "state = torch.randn(256, 11).to(device)  # Batch, state_dim\n",
    "model = Diffusion(loss_type=\"l2\", obs_dim=11, act_dim=2, hid_dim=256, T=100, device=device)\n",
    "result = model(state)  # Sample result\n",
    "\n",
    "loss = model.loss(x, state)\n",
    "\n",
    "print(f\"loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
